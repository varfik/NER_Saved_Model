import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv
from transformers import AutoModel, AutoTokenizer, AutoConfig, BertConfig

from torchcrf import CRF
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler  # Added WeightedRandomSampler
import networkx as nx
import matplotlib.pyplot as plt
import random
import os
import json
from collections import defaultdict
from tqdm.auto import tqdm
from torch.optim import AdamW
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import logging
import unicodedata
import re
import safetensors.torch
from safetensors.torch import save_file

os.environ["TOKENIZERS_PARALLELISM"] = "false"

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.ERROR)

SYMMETRIC_RELATIONS = {'SIBLING', 'SPOUSE', 'RELATIVE'}

ENTITY_TYPES = {
 'PERSON': 1,
 'PROFESSION': 2,
 'ORGANIZATION': 3,
 'FAMILY': 4,
 'LOCATION': 5
}

RELATION_TYPES = {
 'WORKS_AS': 0,
 'MEMBER_OF': 1,
 'FOUNDED_BY': 2,
 'SPOUSE': 3,
 'PARENT_OF': 4,
 'SIBLING': 5,
 'PART_OF': 6,    
 'WORKPLACE': 7,     
 'RELATIVE': 8      
}

VALID_COMB = {
    'WORKS_AS': [('PERSON', 'PROFESSION')],
    'MEMBER_OF': [('PERSON', 'ORGANIZATION'), ('PERSON', 'FAMILY'), ('PROFESSION', 'FAMILY')],
    'FOUNDED_BY': [('ORGANIZATION', 'PERSON'), ('LOCATION', 'PERSON'), ('ORGANIZATION', 'ORGANIZATION'), ('PROFESSION', 'ORGANIZATION')],
    'SPOUSE': [('PERSON', 'PERSON'), ('PROFESSION', 'PROFESSION'), ('PROFESSION', 'PERSON'), ('PERSON', 'PROFESSION')],
    'PARENT_OF': [('PERSON', 'PERSON'), ('PROFESSION', 'PERSON'), ('PERSON', 'PROFESSION')],
    'SIBLING': [('PERSON', 'PERSON'), ('PROFESSION', 'PERSON'), ('PERSON', 'PROFESSION')],
    'PART_OF': [('ORGANIZATION', 'ORGANIZATION'), ('LOCATION', 'LOCATION')],
    'WORKPLACE': [('PERSON', 'ORGANIZATION'), ('PERSON', 'LOCATION'),  ('PROFESSION', 'ORGANIZATION')],
    'RELATIVE': [('PERSON', 'PERSON'), ('PROFESSION', 'PERSON'), ('PERSON', 'PROFESSION')]
}

RELATION_THRESHOLDS = {k: 0.1 for k in RELATION_TYPES}

RELATION_TYPES_INV = {v: k for k, v in RELATION_TYPES.items()}
 
class NERRelationModel(nn.Module):
    def __init__(self, model_name="DeepPavlov/rubert-base-cased", num_ner_labels=len(ENTITY_TYPES)*2+1, num_rel_labels=len(RELATION_TYPES)):
        super().__init__()
        # Initialize NER labels (0=O, 1=B-PER, 2=I-PER, ..., 9=B-LOC, 10=I-LOC)
        self.num_ner_labels = num_ner_labels 
        self.num_rel_labels = num_rel_labels

        # BERT encoder
        self.bert = AutoModel.from_pretrained(model_name, use_safetensors=True)
        hidden_size = self.bert.config.hidden_size

        # NER Head with CRF
        self.ner_classifier = nn.Sequential(
         nn.Linear(hidden_size, 256),
         nn.ReLU(),
         nn.Dropout(0.3),
         nn.Linear(256, num_ner_labels)
        )
        self.crf = CRF(num_ner_labels, batch_first=True)

        # Graph attention network components (GAT)
        # Improved GAT architecture
        self.gat1 = GATConv(
            hidden_size, 
            64, 
            heads=4, 
            dropout=0.3,
            concat=True
        )
        self.norm1 = nn.LayerNorm(64 * 4)
        self.gat2 = GATConv(
            64 * 4, 
            64, 
            heads=1, 
            dropout=0.3,
            concat=False
        )
        self.norm2 = nn.LayerNorm(64)
        # Concatenate heads from first layer

        # Relation classifiers with type-specific architectures
        self.rel_classifier = nn.Sequential(
            nn.Linear(64 * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, len(RELATION_TYPES))
        )
        self._init_weights()


    def _init_weights(self):
        for m in self.rel_classifier:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
                
    def forward(self, input_ids, attention_mask, ner_labels=None, rel_data=None):
        device = input_ids.device
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state

        # Initialize outputs - ИЗМЕНЕНО: теперь список словарей для каждого примера
        rel_probs = [dict() for _ in range(len(rel_data))] if rel_data is not None else []
        total_loss = 0

        # NER prediction with CRF
        ner_logits = self.ner_classifier(sequence_output)
        if ner_labels is not None:
            mask = attention_mask.bool()
            ner_loss = self.crf(ner_logits, ner_labels, mask=mask)
            total_loss -= ner_loss.mean()

        # Early return if no relation data is provided
        if rel_data is None:
            return {
                'ner_logits': ner_logits,
                'rel_probs': rel_probs,
                'loss': total_loss if total_loss != 0 else None
            }

        # Process each sample in batch
        for batch_idx, sample in enumerate(rel_data):
            # Skip if no relation pairs
            if 'pairs' not in sample or len(sample['pairs']) == 0:
                continue

            # Filter valid entities
            valid_entities = [
                e for e in sample['entities'] 
                if isinstance(e, dict) and 'start' in e and 'end' in e and 'type' in e
            ]

            if len(valid_entities) < 2:
                continue

            # Create entity embeddings
            entity_embeddings = []
            entity_types = []
            for e in valid_entities:
                start = min(e['start'], sequence_output.size(1)-1)
                end = min(e['end'], sequence_output.size(1)-1)
                entity_embed = sequence_output[batch_idx, start:end+1].mean(dim=0)
                entity_embeddings.append(entity_embed)
                entity_types.append(e['type'])

            # Build complete graph
            edge_index = torch.tensor([
                [i, j] for i in range(len(valid_entities)) 
                for j in range(len(valid_entities)) if i != j
            ], dtype=torch.long).t().contiguous().to(device)

            x = torch.stack(entity_embeddings).to(device)

            # Apply GAT layers
            x = self.gat1(x, edge_index)
            x = self.norm1(x)
            x = F.elu(x)
            x = F.dropout(x, p=0.3, training=self.training)
            x = self.gat2(x, edge_index)
            x = self.norm2(x)
            x = F.elu(x)


            # Подготовка данных для классификации отношений
            all_pairs = []
            all_labels = []
            for (e1_idx, e2_idx), label in zip(sample['pairs'], sample['labels']):
                if e1_idx >= len(valid_entities) or e2_idx >= len(valid_entities):
                    continue

                pair_features = torch.cat([x[e1_idx], x[e2_idx]]) # итоговый размер [batch, 128]
                all_pairs.append(pair_features)
                all_labels.append(label)  # Теперь label - это просто индекс отношения

            if not all_pairs:
                continue

            # Классификация всех пар сразу
            pair_features = torch.stack(all_pairs)
            logits = self.rel_classifier(pair_features)  # [num_pairs, num_relation_types]
            probs = F.softmax(logits, dim=-1)

            # Расчет потерь
            targets = torch.tensor(all_labels, device=device)  # список индексов
            rel_loss = F.cross_entropy(logits, targets)
            
            total_loss += rel_loss

            rel_probs[batch_idx] = probs.clone().detach()
            
        print(ner_logits.shape)
            

        return {
            'ner_logits': ner_logits,
            'rel_probs': rel_probs,
            'loss': total_loss if total_loss != 0 else None
        }

    def _calculate_pos_weight(self, targets):
        """Вычисляет веса для положительных классов"""
        pos_counts = targets.sum(dim=0)
        neg_counts = targets.shape[0] - pos_counts
        weights = neg_counts / (pos_counts + 1e-10)
        return torch.clamp(weights, min=1.0, max=10.0)
         
 
    def _generate_negative_examples(self, entity_embeddings, entity_types, rel_type, pos_indices=None, ratio=0.5):
        """Генерация негативных примеров с использованием VALID_COMB"""
        device = entity_embeddings.device

        # 1. Получаем допустимые комбинации типов для данного отношения
        valid_combinations = VALID_COMB.get(rel_type, [])

        # 2. Создаем маску допустимых пар
        valid_pairs = set()
        for i, e1 in enumerate(entity_types):
            for j, e2 in enumerate(entity_types):
                if i == j:
                    continue
                # Проверяем все допустимые комбинации для данного отношения
                for comb in valid_combinations:
                    if (e1, e2) == comb or (rel_type in SYMMETRIC_RELATIONS and (e2, e1) == comb):
                        valid_pairs.add((i, j))

        # 3. Исключаем позитивные примеры
        if pos_indices:
            valid_pairs -= pos_indices

        # 4. Для симметричных отношений оставляем только уникальные пары
        if rel_type in SYMMETRIC_RELATIONS:
            valid_pairs = {tuple(sorted(pair)) for pair in valid_pairs}

        # 5. Выбираем случайное подмножество
        valid_pairs = list(valid_pairs)
        num_samples = min(len(valid_pairs), max(5 * len(pos_indices) if pos_indices else 5, 10))
        sampled_pairs = random.sample(valid_pairs, num_samples) if valid_pairs else []

        # 6. Для FOUNDED_BY меняем направление
        if rel_type == 'FOUNDED_BY':
            sampled_pairs = [(j, i) for i, j in sampled_pairs]

        # 7. Подготавливаем результат
        if not sampled_pairs:
            return None

        # Собираем фичи пар
        pair_features = torch.cat([
            torch.stack([entity_embeddings[i] for i, _ in sampled_pairs]),
            torch.stack([entity_embeddings[j] for _, j in sampled_pairs])
        ], dim=1)

        return pair_features, torch.zeros(len(sampled_pairs), device=device)
 
    def save_pretrained(self, save_dir, tokenizer=None):
        """Сохраняет модель, конфигурацию и токенизатор в указанную директорию."""
        os.makedirs(save_dir, exist_ok=True)

        # 1. Сохраняем веса модели в формате safetensors
        model_path = os.path.join(save_dir, "model.safetensors")
        save_file(self.state_dict(), model_path)

        # 2. Сохраняем конфигурацию модели
        config = {
             "model_type": "bert-ner-rel",
             "model_name": getattr(self.bert, "name_or_path", "custom"),
             "num_ner_labels": self.num_ner_labels,
             "num_rel_labels": len(RELATION_TYPES),
             "bert_config": self.bert.config.to_diff_dict(),
             "model_config": {
                 "gat_hidden_size": 64,
                 "gat_heads": 4
             }
        }

        config_path = os.path.join(save_dir, "config.json")
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

         # 3. Сохраняем токенизатор
        if tokenizer is not None:
            tokenizer.save_pretrained(save_dir)
 
    @classmethod
    def from_pretrained(cls, model_dir, device="cuda"):
        import safetensors.torch
        device = torch.device(device)

        # 1. Загружаем конфигурацию
        config_path = os.path.join(model_dir, "config.json")
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Config file not found at {config_path}")

        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        # 2. Инициализируем модель (внутри создается BERT)
        model = cls(
            model_name=config.get("model_name", "DeepPavlov/rubert-base-cased"),
            num_ner_labels=config.get("num_ner_labels", len(ENTITY_TYPES) * 2 + 1),
            num_rel_labels=config.get("num_rel_labels", len(RELATION_TYPES))
        ).to(device)

        # 3. Загружаем веса
        model_path = os.path.join(model_dir, "model.safetensors")
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model weights not found at {model_path}")

        state_dict = safetensors.torch.load_file(model_path, device="cpu")
        model.load_state_dict(state_dict)
        model.to(device)

        model.eval()
        
        print("Случайный вес после загрузки:", model.ner_classifier[0].weight[0][:5])
        return model
    
 
class NERELDataset(Dataset):
    def __init__(self, data_dir, tokenizer, max_length=512):
        self.data_dir = data_dir
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        self.misaligned_entities = 0
        self.total_entities = 0
        self.skipped_relations_due_to_alignment = 0

        self.samples = self._load_samples()
 
    def _load_samples(self):
        samples = []
        for txt_file in os.listdir(self.data_dir):
            if not txt_file.endswith('.txt'):
                continue
 
            ann_path = os.path.join(self.data_dir, txt_file.replace('.txt', '.ann'))
            if not os.path.exists(ann_path):
                continue
 
            with open(os.path.join(self.data_dir, txt_file), 'r', encoding='utf-8') as f:
                text = f.read()
 
            entities, relations = self._parse_ann_file(ann_path, text)
            samples.append({'text': text, 'entities': entities, 'relations': relations})
 
        return samples

    def _find_entity_span(self, entity_text, full_text):
        for match in re.finditer(re.escape(entity_text), full_text):
            return match.start(), match.end()
        return None

 
    def _parse_ann_file(self, ann_path, text):
        entities, relations = [], []
        entity_map = {}

        with open(ann_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                if line.startswith('T'):
                    parts = line.split('\t')
                    if len(parts) < 3:
                        continue

                    entity_id = parts[0]
                    type_and_span = parts[1].split()
                    entity_type = type_and_span[0]

                    if len(type_and_span) < 3:
                        continue
                    # Поддерживаемые типы сущностей
                    if entity_type not in ENTITY_TYPES:
                        continue
                    try:
                        start = int(type_and_span[1])
                        end = int(type_and_span[-1])
                    except ValueError:
                        continue

                    entity_text = parts[2]
                    extracted_text = text[start:end]

                    norm_extracted = unicodedata.normalize("NFC", text[start:end].replace('\u00A0', ' '))
                    norm_expected = unicodedata.normalize("NFC", entity_text.replace('\u00A0', ' '))

                    if norm_extracted != norm_expected:
                        recovered = self._find_entity_span(norm_expected, text)
                        if recovered:
                            start, end = recovered
                        else:
                            # logger.debug(f"Misalignment detected:\n"
                            #              f"  entity_id: {entity_id}\n"
                            #              f"  expected: '{entity_text}'\n"
                            #              f"  found:    '{extracted_text}'\n"
                            #              f"  context:  '{text[start - 20:end + 20].replace(chr(10), '⏎')}'")
                            # logger.warning(f"Entity alignment failed: Entity: '{entity_text}' ({entity_type}), "
                            #                f"Span: {start}-{end}, Text: '{text[start - 10:end + 10]}'")
                            self.misaligned_entities += 1
                            continue

                    entity = {
                        'id': entity_id,
                        'type': entity_type,
                        'start': start,
                        'end': end,
                        'text': entity_text
                    }
                    entities.append(entity)
                    entity_map[entity_id] = entity

                elif line.startswith('R'):
                    parts = line.split('\t')
                    if len(parts) < 2:
                        continue

                    rel_info = parts[1].split()
                    if len(rel_info) < 3:
                        continue

                    rel_type = rel_info[0]
                    arg1 = rel_info[1].split(':')[1] if ':' in rel_info[1] else None
                    arg2 = rel_info[2].split(':')[1] if ':' in rel_info[2] else None

                    # Проверяем существование сущностей
                    if arg1 and arg2 and arg1 in entity_map and arg2 in entity_map:
                        logger.debug(f"Relation: {rel_type} between {arg1} and {arg2}")
                        relations.append({
                            'type': rel_type,
                            'arg1': arg1,
                            'arg2': arg2
                        })
        return entities, relations
 
    def __len__(self):
        return len(self.samples)
 
    def __getitem__(self, idx):
        sample = self.samples[idx]
        text = sample['text']
        text = unicodedata.normalize("NFC", text.replace('\u00A0', ' '))

        # Tokenize with subword information
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            truncation=True,
            padding='max_length',
            return_offsets_mapping=True,
            return_tensors='pt'
        )

        # Initialize NER labels (0=O, 1=B-PER, 2=I-PER, 3=B-PROF, 4=I-PROF, 5=B-ORGANIZATION, 6=I-ORGANIZATION, 7=B-FAMILY, 8=I-FAMILY)
        ner_labels = torch.zeros(self.max_length, dtype=torch.long)
        offset_mapping = encoding['offset_mapping'][0]
        token_entities = []

        # Align entities with tokenization
        for entity in sample['entities']:
            matched_tokens = []

            # Find token spans for entity
            start_token = end_token = None
            for i, (start, end) in enumerate(offset_mapping):
                if start == end:
                    continue  # спецтокены
                if start < entity['end'] and end > entity['start']:
                    matched_tokens.append(i)

            # if not matched_tokens:
            #     recovered = self._find_best_span(entity['text'], text, entity['start'])
            #     if recovered:
            #         entity['start'], entity['end'] = recovered
            #         for i, (start, end) in enumerate(offset_mapping):
            #             if start < entity['end'] and end > entity['start']:
            #                 matched_tokens.append(i)

            if not matched_tokens:
                # logger.warning(f"Entity alignment failed: Entity: '{entity['text']}' ({entity['type']}), "
                #                f"Span: {entity['start']}-{entity['end']}, "
                #                f"Text segment: '{text[entity['start']:entity['end']]}'")
                continue

            ent_type_id = ENTITY_TYPES[entity['type']]
            b_label = ent_type_id * 2 - 1
            i_label = ent_type_id * 2
            ner_labels[matched_tokens[0]] = b_label
            for idx in matched_tokens[1:]:
                ner_labels[idx] = i_label
            token_entities.append({
                'start': matched_tokens[0],
                'end': matched_tokens[-1],
                'type': entity['type'],
                'id': entity['id']
            })

        # Prepare relation data
        rel_data = {
            'entities': token_entities,
            'pairs': [],
            'labels': [],
            'rel_types': []
        }

        token_entity_id_to_idx = {e['id']: i for i, e in enumerate(token_entities)}
        num_entities = len(token_entities)
        positive_pairs = set()
        
        # 1. Добавляем позитивные пары
        for relation in sample['relations']:
            if relation['type'] not in RELATION_TYPES:
                continue
            idx1 = token_entity_id_to_idx.get(relation['arg1'], -1)
            idx2 = token_entity_id_to_idx.get(relation['arg2'], -1)
            if idx1 == -1 or idx2 == -1:
                self.skipped_relations_due_to_alignment += 1
                continue
            if relation['type'] in SYMMETRIC_RELATIONS:
                idx1, idx2 = sorted([idx1, idx2])
            pair = (idx1, idx2)
            if pair not in positive_pairs:
                rel_data['pairs'].append(pair)
                rel_data['labels'].append(RELATION_TYPES[relation['type']])
                rel_data['rel_types'].append(relation['type'])
                positive_pairs.add(pair)

        # 2. Добавляем негативные пары
        negative_pairs = set()
        
        for i, e1 in enumerate(token_entities):
            for j, e2 in enumerate(token_entities):
                if i == j:
                    continue

                # Проверяем все возможные допустимые комбинации отношений
                for rel_type, combs in VALID_COMB.items():
                    for comb in combs:
                        if (e1['type'], e2['type']) == comb or (rel_type in SYMMETRIC_RELATIONS and (e2['type'], e1['type']) == comb):
                            pair = tuple(sorted([i, j])) if rel_type in SYMMETRIC_RELATIONS else (i, j)
                            if pair not in positive_pairs:
                                negative_pairs.add(pair)

        # Ограничиваем количество негативных примеров
        max_negatives = min(len(positive_pairs) * 3, 20)  # 3:1 соотношение
        negative_pairs = random.sample(list(negative_pairs), max_negatives) if negative_pairs else []
                

        output = {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'ner_labels': ner_labels,
            'rel_data': rel_data,
            'text': text,
            'offset_mapping': encoding['offset_mapping'].squeeze(0)
        }
        
        # logger.error(f"Output keys: {list(output.keys())}")
        # logger.error(f"input_ids shape: {output['input_ids'].shape}")
        # logger.error(f"ner_labels shape: {output['ner_labels'].shape}")
        # logger.error(f"offset_mapping shape: {output['offset_mapping'].shape}")
        # logger.error(f"rel_data pairs: {output['rel_data']['pairs']}")
        # logger.error(f"rel_data labels: {output['rel_data']['labels']}")

        return output
 
def collate_fn(batch):
     # All elements already padded to max_length
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    ner_labels = torch.stack([item['ner_labels'] for item in batch])
    offset_mapping = torch.stack([item['offset_mapping'] for item in batch])
 
    device = input_ids.device
 
    rel_data = []
     # Собираем rel_data как список словарей
    for item in batch:
        rel_entry = {
             'entities': item['rel_data']['entities'],
             'pairs': torch.tensor(item['rel_data']['pairs'], dtype=torch.long) if item['rel_data']['pairs'] else torch.zeros((0, 2), dtype=torch.long, device=device),
             'labels': torch.tensor(item['rel_data']['labels'], dtype=torch.long) if item['rel_data']['labels'] else torch.zeros(0, dtype=torch.long, device=device),
             'rel_types': item['rel_data'].get('rel_types', ['NO_REL'] * len(item['rel_data']['labels']))
        }
        rel_data.append(rel_entry)
 
    return {
         'input_ids': input_ids,
         'attention_mask': attention_mask,
         'ner_labels': ner_labels,
         'rel_data': rel_data,
         'texts': [item['text'] for item in batch],
         'offset_mapping': offset_mapping
    }
 
def train_model():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Инициализация модели и токенизатора
    tokenizer = AutoTokenizer.from_pretrained("DeepPavlov/rubert-base-cased")
    model = NERRelationModel().to(device)

    # Загрузка данных
    train_dataset = NERELDataset("NEREL/NEREL-v1.1/train", tokenizer)

    # Create weighted sampler to balance relation examples
    sample_weights = []
    for sample in train_dataset:
        has_relations = len(sample['rel_data']['labels']) > 0
        sample_weights.append(1.0 if has_relations else 0.3)

    sampler = WeightedRandomSampler(sample_weights, len(train_dataset), replacement=True)
    train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=collate_fn, sampler=sampler, pin_memory=True)

    # Optimizer with different learning rates
    optimizer = AdamW([
        {'params': model.bert.parameters(), 'lr': 3e-5},
        {'params': model.ner_classifier.parameters(), 'lr': 5e-5},
        {'params': model.crf.parameters(), 'lr': 5e-5},
        {'params': model.gat1.parameters(), 'lr': 5e-5},
        {'params': model.gat2.parameters(), 'lr': 5e-5},
        {'params': model.rel_classifier.parameters(), 'lr': 5e-5}
    ])

    # Training loop
    best_ner_f1 = 0
    for epoch in range(2):
        model.train()
        epoch_loss = 0
        ner_correct = ner_total = 0
        rel_correct = rel_total = 0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            print(batch['rel_data'][0]['pairs'])  # Проверить наличие пар
            print(batch['rel_data'][0]['labels']) # Проверить наличие меток
            optimizer.zero_grad()

            # Перенос данных на устройство
            input_ids = batch['input_ids'].to(device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(device)
            ner_labels = batch['ner_labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                ner_labels=ner_labels,
                rel_data=batch['rel_data']
            )

            if outputs['loss'] is not None:
                outputs['loss'].backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                epoch_loss += outputs['loss'].item()

            # NER metrics
            with torch.no_grad():
                mask = attention_mask.bool()
                ner_preds = model.crf.decode(outputs['ner_logits'], mask=mask)

                # Перебираем каждый пример в батче
                for i in range(len(ner_preds)):
                    # Получаем длину последовательности без паддинга
                    seq_len = mask[i].sum().item()
                    # Берем только нужные элементы (без паддинга)
                    pred = torch.tensor(ner_preds[i][:seq_len], device=device)
                    true = ner_labels[i][:seq_len]

                    ner_correct += (pred == true).sum().item()
                    ner_total += seq_len

            # Relation metrics
            if outputs.get('rel_probs'):
                for batch_idx, batch_item in enumerate(batch['rel_data']):
                    pairs = batch_item['pairs']
                    labels = batch_item['labels']

                    if len(pairs) == 0:
                        continue

                    # Получаем предсказанные вероятности для этого примера
                    probs = outputs['rel_probs'][batch_idx]  # [num_pairs, num_rel_types]

                    # Получаем предсказанные классы
                    preds = torch.argmax(probs, dim=1)

                    # Сравниваем с истинными метками
                    labels = labels.clone().detach().to(preds.device)
                    correct = (preds == labels).sum().item()
                    total = len(labels)

                    rel_correct += correct
                    rel_total += total

        # Evaluation metrics
        ner_acc = ner_correct / ner_total if ner_total > 0 else 0
        rel_acc = rel_correct / rel_total if rel_total > 0 else 0

        print(f"\nEpoch {epoch+1} Results:")
        print(f"Loss: {epoch_loss/len(train_loader):.4f}")
        print(f"NER Accuracy: {ner_acc:.2%} ({ner_correct}/{ner_total})")
        print(f"Relation Accuracy: {rel_acc:.2%} ({rel_correct}/{rel_total})")


    print("Случайный вес до сохранения:", model.ner_classifier[0].weight[0][:5])
        
    save_dir = "saved_model"
    tokenizer.save_pretrained(save_dir)
    model.save_pretrained(save_dir, tokenizer=tokenizer)

    print(f"Model saved to {save_dir}")

    return model, tokenizer
 
def predict(text, model, tokenizer, device="cuda", relation_threshold=None):
    # Tokenize input with offset mapping
    encoding = tokenizer(text, return_tensors="pt", return_offsets_mapping=True).to(device)
    default_thresholds = {k: 0.5 for k in RELATION_TYPES}
    if relation_threshold is not None:
        default_thresholds.update(relation_threshold)
    relation_threshold = default_thresholds
 
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    offset_mapping = encoding['offset_mapping'][0]
 
    with torch.no_grad():
        outputs = model(input_ids, attention_mask)
 
    # Decode NER with CRF
    mask = attention_mask.bool()
    ner_preds = model.crf.decode(outputs['ner_logits'], mask=mask)[0]
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0], skip_special_tokens=True)
    
    print("TOKENS:", tokens)
    print("NER PREDS:", ner_preds)

     # Extract entities
    entities = []
    current_entity = None
    entity_id = 0
    
    print("NER predictions:")
    print("Tokens:", tokens)
    print("Preds:", ner_preds)

    for i, (token, pred) in enumerate(zip(tokens, ner_preds)):
        if token in ['[CLS]', '[SEP]', '[PAD]']:
            if current_entity:
                entities.append(current_entity)
                current_entity = None
            continue
 
        # Get the character offsets for this token
        token_start, token_end = offset_mapping[i]
 
         # Handle entity extraction
        if pred % 2 == 1:  # Beginning of entity (B- tag)
            if current_entity:
                entities.append(current_entity)
 
            entity_type = None
            if pred == 1: entity_type = "PERSON"
            elif pred == 3: entity_type = "PROFESSION"
            elif pred == 5: entity_type = "ORGANIZATION"
            elif pred == 7: entity_type = "FAMILY"
            elif pred == 9: entity_type = "LOCATION"
 
            if entity_type:
                current_entity = {
                     'id': f"T{entity_id}",
                     'type': entity_type,
                     'start': i,
                     'end': i,
                     'start_char': token_start,
                     'end_char': token_end,
                     'token_ids': [i]
                }
                entity_id += 1
 
        elif pred % 2 == 0 and pred != 0:  # Inside of entity (I- tag)
            if current_entity:
                # Check if this continues the current entity
                expected_type = None
                if pred == 2: expected_type = "PERSON"
                elif pred == 4: expected_type = "PROFESSION"
                elif pred == 6: expected_type = "ORGANIZATION"
                elif pred == 8: expected_type = "FAMILY"
                elif pred == 10: expected_type = "LOCATION"
 
                if expected_type and current_entity['type'] == expected_type:
                    current_entity['end'] = i
                    current_entity['end_char'] = token_end
                    current_entity['token_ids'].append(i)
                else:
                     # Type mismatch, save current and start new
                    entities.append(current_entity)
                    current_entity = None
        else:  # O (outside)
            if current_entity:
                entities.append(current_entity)
                current_entity = None
 
     # Add the last entity if exists
    if current_entity:
        entities.append(current_entity)
 
     # Now get the actual text for each entity
    for entity in entities:
        entity['text'] = text[entity['start_char']:entity['end_char']]
 
    if len(entities) < 2:  # Не может быть отношений, если меньше 2 сущностей
        return {
             'text': text,
             'entities': entities,
             'relations': []
        }
    
 
     # Extract relations
    
    entity_map = {e['id']: e for e in entities}
 
    if len(entities) >= 2:
        sequence_output = model.bert(input_ids, attention_mask).last_hidden_state
 
         # Create entity embeddings
        entity_embeddings = []
        for e in entities:
             # Get all token embeddings for this entity
            token_embeddings = sequence_output[0, e['token_ids']]
            # Average them
            entity_embed = token_embeddings.mean(dim=0)
            entity_embeddings.append(entity_embed)
 
        entity_embeddings = torch.stack(entity_embeddings).to(device)
 
         # Build complete graph
        edge_index = torch.tensor([
            [i, j] for i in range(len(entities)) 
            for j in range(len(entities)) if i != j
        ], dtype=torch.long).t().contiguous().to(device)
 
        # Apply GAT
        x = model.gat1(entity_embeddings, edge_index)
        x = F.elu(x)
        x = F.dropout(x, p=0.3, training=False)
        x = model.gat2(x, edge_index)
        x = F.elu(x)
 
        # Predict all possible relations
        relations = []
        for i, e1 in enumerate(entities):
            for j, e2 in enumerate(entities):
                if i != j:
                    pair_features = torch.cat([x[i], x[j]])
                    logits = model.rel_classifier(pair_features.unsqueeze(0))
                    probs = F.softmax(logits, dim=-1).squeeze()
                    
                    print(f"\nPAIR: {e1['text']} -> {e2['text']}")
                    print("Relation probs:", {RELATION_TYPES_INV[i]: float(p) for i, p in enumerate(probs)})

                    # Находим наиболее вероятный тип отношения
                    max_prob, max_idx = probs.max(dim=0)
                    rel_type = RELATION_TYPES_INV[max_idx.item()]

                    if max_prob > relation_threshold[rel_type]:
                        relations.append({
                            'type': rel_type,
                            'arg1': entity_map[entities[i]['id']],
                            'arg2': entity_map[entities[j]['id']],
                            'confidence': max_prob.item()
                        })
          

     # Remove duplicates and keep highest confidence
    print("Raw entities:", entities)
    print("Raw relations:", relations)
    
    
    unique_relations = {}
    for rel in relations:
        key = (rel['arg1']['id'], rel['arg2']['id'], rel['type'])  # <-- исправлено
        if key not in unique_relations or rel['confidence'] > unique_relations[key]['confidence']:
            unique_relations[key] = rel
 
     # Sort by confidence
    sorted_relations = sorted(unique_relations.values(), 
                              key=lambda x: x['confidence'], reverse=True)
 
    return {
         'text': text,
         'entities': entities,
         'relations': sorted_relations
    }
 
if __name__ == "__main__":
    model, tokenizer = train_model()
    test_texts = [
        "Эмир Катара встретится с членами королевской семьи.Эмир Катара шейх Хамад бен Халиф Аль Тани встретится в понедельник с членами королевской семьи и высокопоставленными чиновниками страны на фоне слухов о том, что он намерен передать власть сыну — наследному принцу шейху Тамиму, передает агентство Рейтер со ссылкой на катарский телеканал 'Аль-Джазира'. 'Аль-Джазира', в свою очередь, ссылается на 'надежный источник в Катаре', но не приводит каких-либо деталей. Ранее в этом месяце в дипломатических кругах появились слухи, что эмир Катара, которому сейчас 61 год, рассматривает возможность передачи власти 33-летнему наследному принцу, отмечает агентство. При этом также предполагается, что в отставку подаст влиятельный премьер-министр и министр иностранных дел Катара шейх Хамад бен Джасем Аль Тани. По данным агентства, дипломаты западных и арабских стран оценивают такое решение как попытку осторожной передачи власти более молодому поколению правителей. Ранее новостной портал 'Элаф' отмечал, что перемены во властных структурах Катара могут произойти уже в конце июня. Согласно информации агентства Франс Пресс, Тамим бен Хамад Аль Тани родился в 1980 году и является вторым сыном эмира и его второй жены Мозы бинт Нассер. Наследный принц занимает офицерский пост в катарской армии, а также является главой Олимпийского комитета страны.",
        "Айрат Мурзагалиев, заместитель начальника управления президента РФ, встретился с главой администрации Уфы.",
        "Иван Петров работает программистом в компании Яндекс.",
        "Доктор Сидоров принял пациентку Ковалеву в городской больнице.",
        "Директор сводного экономического департамента Банка России Надежда Иванова назначена также на должность заместителя председателя ЦБ, сообщил в четверг регулятор.",
        "Дмитрий работает в организации 'ЭкоФарм'",
        "Компания 'Технологии будущего' является частью крупной корпорации, расположенной в Санкт-Петербурге",
        "Анна занимает должность главного врача в больнице 'Здоровье'."
    ]

 
    for text in test_texts:
         print("\n" + "="*80)
         print(f"Processing text: '{text}'")
         result = predict(text, model, tokenizer)
         print("\nEntities:")
         for e in result['entities']:
             print(f"{e['type']}: {e['text']}")
         print("\nRelations:")
         for r in result['relations']:
             print(f"{r['type']}: {r['arg1']['text']} -> {r['arg2']['text']} (conf: {r['confidence']:.2f})")
 
     # Для загрузки модели
    tokenizer = AutoTokenizer.from_pretrained("saved_model")
    model = NERRelationModel.from_pretrained("saved_model")
 
     # Использование модели
    result = predict("Компания 'Технологии будущего' является частью крупной корпорации, расположенной в Санкт-Петербурге", loaded_model, loaded_tokenizer)
    print("Сущности:")
    for e in result['entities']:
         print(f"{e['type']}: {e['text']} (позиция: {e['start_char']}-{e['end_char']})")
 
    print("\nОтношения:")
    for r in result['relations']:
         print(f"{r['type']}: {r['arg1']['text']} -> {r['arg2']['text']} (confidence: {r['confidence']:.2f})")
